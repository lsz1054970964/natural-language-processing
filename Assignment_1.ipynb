{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b233a774-1ba7-499b-82ba-e64798063238",
   "metadata": {
    "id": "b233a774-1ba7-499b-82ba-e64798063238"
   },
   "source": [
    "*Prepared for the course \"TSTS22: Natural Language Processing and Text Mining\" at Jönköping University, Teacher: [Marcel Bollmann](marcel.bollmann@ju.se)*\n",
    "\n",
    "# Assignment 1: News Topic Classification\n",
    "\n",
    "In this assignment, you will train classifiers to predict the topic area of a news article based on its headline.\n",
    "\n",
    "The dataset was prepared from a public domain (CC-0) dataset of news headlines on Kaggle, and contains 10,000 articles for training – consisting of a headline (_title_) and one of 8 different topic areas (_topic_) – and 2,000 articles for validation.  You can see some stats about the dataset in the code cells below.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "This assignment consists of **three parts.**  In each part, you'll find a <span style=\"background-color:#008148; padding:4px 8px; border-radius:4px; color:#F8F0E3\">green box</span> that indicates where your own solution should begin, and an <span style=\"background-color:#EF8A17; padding:4px 8px; border-radius:4px; color:#F8F0E3\">orange box</span> that is followed by some evaluation code which you should **not** modify.\n",
    "\n",
    "### Grading\n",
    "\n",
    "- This assignment is graded Pass/Fail.\n",
    "\n",
    "- To _pass_ this assignment, you must provide a working solution for _all parts_ of the assignment. This means that:\n",
    "    - Your notebook should run from start to finish without errors.\n",
    "    - Your solutions should fulfill the requirements described in the parts below.\n",
    "    - The provided evaluation code must not be modified.\n",
    "\n",
    "- - - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4IVFY_NBQVt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c4IVFY_NBQVt",
    "outputId": "2375ee19-34c5-42c2-a005-9d3e3f1ef52a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.7/dist-packages (12.5.1)\n",
      "Requirement already satisfied: commonmark<0.10.0,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from rich) (0.9.1)\n",
      "Requirement already satisfied: typing-extensions<5.0,>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from rich) (4.1.1)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from rich) (2.6.1)\n"
     ]
    }
   ],
   "source": [
    "#!python -m pip install rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "CmvJ5Vo1ENNX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CmvJ5Vo1ENNX",
    "outputId": "cd66cc0f-833d-438f-f40e-6187f4cd7c3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Connected to google drive\n",
    "# I used the goolge colab for this assignment. It could be commented when running in jupyter\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0b38e62-67f6-4e8d-903a-aa86f1d46ffa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c0b38e62-67f6-4e8d-903a-aa86f1d46ffa",
    "outputId": "1865afa1-e28c-4de1-b7db-34b9b4643738"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Imports and loading the dataset\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import rich\n",
    "import rich.progress\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# These variables are used later -- DO NOT MODIFY THEM.\n",
    "# Assign new variable names if you need to modify the data\n",
    "# in some way.\n",
    "df_train = pd.read_csv(\"newstopics_train.csv\")\n",
    "#df_train = pd.read_csv(\"drive/MyDrive/TSTS22_Assignment1/newstopics_train.csv\")\n",
    "df_val = pd.read_csv(\"newstopics_val.csv\")\n",
    "#df_val = pd.read_csv(\"drive/MyDrive/TSTS22_Assignment1/newstopics_val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca99a57f-9026-490a-9412-25c89311efe1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "ca99a57f-9026-490a-9412-25c89311efe1",
    "outputId": "a529d090-7068-4f4b-b599-4d70f84c6866"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-b38b31b6-b871-46ef-9f17-167c30c6fb2f\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Former AMP chairman says culture long an issue...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nagelsmann defends Guardiola after Man City's ...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tennessee bans on-campus tailgating, expecting...</td>\n",
       "      <td>sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Currys PC World owner slashes 800 jobs as coro...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Royal National Park: Police release descriptio...</td>\n",
       "      <td>nation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b38b31b6-b871-46ef-9f17-167c30c6fb2f')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-b38b31b6-b871-46ef-9f17-167c30c6fb2f button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-b38b31b6-b871-46ef-9f17-167c30c6fb2f');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                               title     topic\n",
       "0  Former AMP chairman says culture long an issue...  business\n",
       "1  Nagelsmann defends Guardiola after Man City's ...    sports\n",
       "2  Tennessee bans on-campus tailgating, expecting...    sports\n",
       "3  Currys PC World owner slashes 800 jobs as coro...  business\n",
       "4  Royal National Park: Police release descriptio...    nation"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8416ee08-3b07-4011-8eaf-364e29a93228",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8416ee08-3b07-4011-8eaf-364e29a93228",
    "outputId": "d54eeea5-a15d-4664-d5e6-364b0c813f37"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "business         1379\n",
       "sports           1379\n",
       "nation           1379\n",
       "health           1379\n",
       "entertainment    1379\n",
       "technology       1379\n",
       "world            1379\n",
       "science           347\n",
       "Name: topic, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[\"topic\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ee43cc-d50f-4de0-a7d4-45e3fda576c5",
   "metadata": {
    "id": "88ee43cc-d50f-4de0-a7d4-45e3fda576c5",
    "tags": []
   },
   "source": [
    "- - -\n",
    "\n",
    "## Part 1: Linear bag-of-words\n",
    "\n",
    "In this part, your task is to build a simple **linear bag-of-words classifier.**\n",
    "\n",
    "The classifier should take a text string as input and predict a genre label as output. All of the functionality you need can be found in Scikit-learn, though you may also use other libraries such as NLTK if you like.\n",
    "\n",
    "<div style=\"background-color:#008148; padding:4px 8px; border-radius:4px; color:#F8F0E3\">\n",
    "    <strong>Modify the cell(s) below with your implementation.</strong>\n",
    "</div>\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc7d068a",
   "metadata": {
    "id": "dc7d068a"
   },
   "outputs": [],
   "source": [
    "# Convert to lowercase, strip and remove punctuations\n",
    "def preprocess(text):\n",
    "    text = text.lower() \n",
    "    text=text.strip()  \n",
    "    text=re.compile('<.*?>').sub('', text) \n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  \n",
    "    text = re.sub('\\s+', ' ', text)  \n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',text) \n",
    "    text=re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    text = re.sub(r'\\d',' ',text) \n",
    "    text = re.sub(r'\\s+',' ',text) \n",
    "    return text\n",
    "\n",
    " \n",
    "# Romove stopword\n",
    "def stopword(string):\n",
    "    a= [i for i in string.split() if i not in stopwords.words('english')]\n",
    "    return ' '.join(a)\n",
    "\n",
    "#LEMMATIZATION\n",
    "# Initialize the lemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    " \n",
    "# This is a helper function to map NTLK position tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Tokenize the sentence\n",
    "def lemmatizer(string):\n",
    "    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags\n",
    "    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token\n",
    "    return \" \".join(a)\n",
    "\n",
    "def finalpreprocess(string):\n",
    "    return lemmatizer(stopword(preprocess(string)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "H_xdY2Ud6xX_",
   "metadata": {
    "id": "H_xdY2Ud6xX_"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def make_linear_classifier():\n",
    "    class MulticlassClassification:\n",
    "        def __init__(self):\n",
    "            self.models = []\n",
    "            global tfidf_vectorizer\n",
    "            count_vectorizer = CountVectorizer()\n",
    "            \n",
    "        def fit(self, X, y):\n",
    "            \"\"\"\n",
    "            Fits each model\n",
    "            \"\"\"\n",
    "            X = X.apply(lambda x: finalpreprocess(x))\n",
    "            X = count_vectorizer.fit_transform(X).toarray()\n",
    "\n",
    "            for y_i in np.unique(y):\n",
    "                # y_i - positive class for now\n",
    "                # All other classes except y_i are negative\n",
    "\n",
    "                # Choose x where y is positive class\n",
    "                x_true = X[y == y_i]\n",
    "                # Choose x where y is negative class\n",
    "                x_false = X[y != y_i]\n",
    "                # Concatanate\n",
    "                x_true_false = np.vstack((x_true, x_false))\n",
    "\n",
    "                # Set y to 1 where it is positive class\n",
    "                y_true = np.ones(x_true.shape[0])\n",
    "                # Set y to 0 where it is negative class\n",
    "                y_false = np.zeros(x_false.shape[0])\n",
    "                # Concatanate\n",
    "                y_true_false = np.hstack((y_true, y_false))\n",
    "\n",
    "                # Fit model and append to models list\n",
    "                model = LogisticRegression()\n",
    "                model.fit(x_true_false, y_true_false)\n",
    "                self.models.append([y_i, model])\n",
    "\n",
    "\n",
    "        def predict(self, X):\n",
    "            X = X.apply(lambda x: finalpreprocess(x))\n",
    "            X = count_vectorizer.transform(X).toarray()\n",
    "            y_pred = [[label, model.predict(X)] for label, model in self.models]\n",
    "\n",
    "            output = []\n",
    "\n",
    "            for i in range(X.shape[0]):\n",
    "                max_label = None\n",
    "                max_prob = -10**5\n",
    "                for j in range(len(y_pred)):\n",
    "                    prob = y_pred[j][1][i]\n",
    "                    if prob > max_prob:\n",
    "                        max_label = y_pred[j][0]\n",
    "                        max_prob = prob\n",
    "                output.append(max_label)\n",
    "\n",
    "        return output\n",
    "\n",
    "    return MulticlassClassification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2Meb4ZiPVkDR",
   "metadata": {
    "id": "2Meb4ZiPVkDR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9db157b7-501b-4c38-af16-3d783a882376",
   "metadata": {
    "id": "9db157b7-501b-4c38-af16-3d783a882376"
   },
   "source": [
    "<div style=\"background-color:#EF8A17; padding:4px 8px; border-radius:4px; color:#F8F0E3; margin-bottom:1em;\">\n",
    "  <strong>Do NOT modify the code cell below.</strong>\n",
    "</div>\n",
    "\n",
    "Run the cell below to fit and evaluate your model. Your goal is to obtain **a weighted F1-score of 0.6 or more**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "537fa8f5-6355-40aa-99a2-7dce76677205",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 288
    },
    "id": "537fa8f5-6355-40aa-99a2-7dce76677205",
    "outputId": "98759eda-70cf-4523-a569-a4b2b349f84c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "     business       0.27      0.92      0.42       276\n",
      "entertainment       0.92      0.54      0.68       276\n",
      "       health       0.78      0.60      0.68       276\n",
      "       nation       0.68      0.30      0.42       275\n",
      "      science       0.94      0.42      0.58        69\n",
      "       sports       0.96      0.70      0.81       276\n",
      "   technology       0.95      0.67      0.79       276\n",
      "        world       0.73      0.38      0.50       276\n",
      "\n",
      "     accuracy                           0.58      2000\n",
      "    macro avg       0.78      0.56      0.61      2000\n",
      " weighted avg       0.76      0.58      0.61      2000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Weighted F1-score: <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.611</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Weighted F1-score: \u001b[1;32m0.611\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def fit_and_evaluate(clf):\n",
    "    clf.fit(df_train[\"title\"], df_train[\"topic\"])\n",
    "    y_pred = clf.predict(df_val[\"title\"])\n",
    "    print(metrics.classification_report(df_val[\"topic\"], y_pred))\n",
    "    wf1 = metrics.f1_score(df_val[\"topic\"], y_pred, average=\"weighted\")\n",
    "    color = \"green\" if wf1 >= .6 else \"red\"\n",
    "    rich.print(f\"Weighted F1-score: [bold {color}]{wf1:.3f}[/]\")\n",
    "\n",
    "fit_and_evaluate(make_linear_classifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18226461-eb35-4b71-93d3-9a7259ce0033",
   "metadata": {
    "id": "18226461-eb35-4b71-93d3-9a7259ce0033"
   },
   "source": [
    "- - - \n",
    "\n",
    "## Part 2: Neural text classification\n",
    "\n",
    "In this part, your task is to build a simple **neural network** that implements a text classification model.  Concretely, your network needs to consist of:\n",
    "\n",
    "1. An **embedding layer** that maps tokens to an embedding space;\n",
    "2. Any number of **intermediate layers** that ultimately result in a 1-dimensional vector representation;\n",
    "3. A **final linear layer** that outputs a softmax over the number of class labels.\n",
    "\n",
    "You **may _not_ use any recurrent layers or transformer architectures (e.g. multi-head attention)**, but you are free to otherwise experiment with any combination of layers to improve your model, such as dropout, extra linear layers, convolutional layers, etc.\n",
    "\n",
    "The model that you build should **not need a GPU** to train; make sure to keep it small enough so that it doesn't take longer than 10 minutes to train on your CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09460d0d-3ceb-4fe1-9010-00f4d87984dd",
   "metadata": {
    "id": "09460d0d-3ceb-4fe1-9010-00f4d87984dd"
   },
   "source": [
    "<div style=\"background-color:#008148; padding:4px 8px; border-radius:4px; color:#F8F0E3; margin-bottom:1em;\">\n",
    "    <strong>Modify the cell(s) below with your implementation.</strong>\n",
    "</div>\n",
    "\n",
    "The following cell already imports some names from TensorFlow and Keras for convenience; you are free to change this to another library, such as PyTorch, if you prefer that.  Just make sure to _use the same function names_ as below, so that the evaluation code in the cell below still works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "zk7VCZgtAfQm",
   "metadata": {
    "id": "zk7VCZgtAfQm"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "E7Igna1CAhR2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "id": "E7Igna1CAhR2",
    "outputId": "f35a6148-e932-4a11-a389-d532973066ec"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Using TensorFlow <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2.8</span>.<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Using TensorFlow \u001b[1;36m2.8\u001b[0m.\u001b[1;36m2\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rich.print(f\"Using TensorFlow {tf.__version__}\")\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # suppress low-level TF warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "tHBPo0i_PpdW",
   "metadata": {
    "id": "tHBPo0i_PpdW"
   },
   "outputs": [],
   "source": [
    "sentences = df_train['title'].apply(lambda x: finalpreprocess(x))\n",
    "y = df_train['topic']\n",
    "y_sentences = df_val['title'].apply(lambda x: finalpreprocess(x))\n",
    "y_val = df_val['topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "SoSZV3P1BPTa",
   "metadata": {
    "id": "SoSZV3P1BPTa"
   },
   "outputs": [],
   "source": [
    "# Convert topic label into vector variables\n",
    "encoder = LabelBinarizer()\n",
    "transfomed_label = encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fVNhfXouBW3r",
   "metadata": {
    "id": "fVNhfXouBW3r"
   },
   "outputs": [],
   "source": [
    "# Split train and test set\n",
    "sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, transfomed_label, test_size=0.25, random_state=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "VrLdL7XvBeFb",
   "metadata": {
    "id": "VrLdL7XvBeFb"
   },
   "outputs": [],
   "source": [
    "# Word embedding\n",
    "# Represent words in dataset with vectors\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(sentences_train)\n",
    "X_train = tokenizer.texts_to_sequences(sentences_train)\n",
    "X_test = tokenizer.texts_to_sequences(sentences_test)\n",
    "X_val = tokenizer.texts_to_sequences(y_sentences)\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "X0UIaHQ6Bh-A",
   "metadata": {
    "id": "X0UIaHQ6Bh-A"
   },
   "outputs": [],
   "source": [
    "# Pad sequence with Keras\n",
    "maxlen = 100\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "X_val = pad_sequences(X_val, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8291b9a-9810-46c4-9f9d-65745a5215bc",
   "metadata": {
    "id": "f8291b9a-9810-46c4-9f9d-65745a5215bc"
   },
   "outputs": [],
   "source": [
    "def make_neural_classifier():\n",
    "    \"\"\"This function should instantiate and return your\n",
    "       neural text classification model.\"\"\"\n",
    "    #raise NotImplementedError()\n",
    "    #model = tf.keras.Sequential([...])\n",
    "    #model.compile(...)\n",
    "    #return model\n",
    "    embedding_dim = 50\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(input_dim=vocab_size, \n",
    "                              output_dim=embedding_dim, \n",
    "                              input_length=maxlen))\n",
    "    model.add(layers.Conv1D(50, 5, activation=\"relu\"))\n",
    "    model.add(layers.GlobalMaxPool1D())\n",
    "    model.add(layers.Dense(10, activation='relu'))\n",
    "    model.add(layers.Dense(8, activation='softmax'))\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def fit_and_predict(model):\n",
    "    \"\"\"This function should take a model object, \n",
    "       fit the model on the training set,\n",
    "       and return predictions on the validation set.\"\"\"\n",
    "    #raise NotImplementedError()\n",
    "\n",
    "    model.fit(X_train, y_train,\n",
    "                    epochs=10,\n",
    "                    verbose=True,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    batch_size=10)\n",
    "    \n",
    "    y_prediction = (model.predict(X_val) > 0.5).astype(\"int32\")\n",
    "    y_pred = encoder.inverse_transform(y_prediction)\n",
    "\n",
    "    return y_pred\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bNvJK867Bcdn",
   "metadata": {
    "id": "bNvJK867Bcdn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aaa6fe3c-542c-46ad-9f9b-c22429690504",
   "metadata": {
    "id": "aaa6fe3c-542c-46ad-9f9b-c22429690504"
   },
   "source": [
    "# <div style=\"background-color:#EF8A17; padding:4px 8px; border-radius:4px; color:#F8F0E3; margin-bottom:1em;\">\n",
    "  <strong>Do NOT modify the code cell below.</strong>\n",
    "</div>\n",
    "\n",
    "Run the cell below to fit and evaluate your model. Your goal is to implement a working neural classifier that **trains in 10 minutes or less on CPU** and obtains **a weighted F1-score of 0.6 or more**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "82c4d1f1-3ae2-4039-b0a7-c20ae5e98580",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "id": "82c4d1f1-3ae2-4039-b0a7-c20ae5e98580",
    "outputId": "4613146a-fef9-48b9-a6c2-661cf4e426a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "750/750 [==============================] - 31s 40ms/step - loss: 1.6724 - accuracy: 0.3627 - val_loss: 1.1916 - val_accuracy: 0.6020\n",
      "Epoch 2/10\n",
      "750/750 [==============================] - 30s 39ms/step - loss: 0.8411 - accuracy: 0.7208 - val_loss: 1.0391 - val_accuracy: 0.6556\n",
      "Epoch 3/10\n",
      "750/750 [==============================] - 33s 44ms/step - loss: 0.4489 - accuracy: 0.8621 - val_loss: 1.1498 - val_accuracy: 0.6548\n",
      "Epoch 4/10\n",
      "750/750 [==============================] - 28s 37ms/step - loss: 0.2402 - accuracy: 0.9315 - val_loss: 1.3341 - val_accuracy: 0.6552\n",
      "Epoch 5/10\n",
      "750/750 [==============================] - 29s 39ms/step - loss: 0.1279 - accuracy: 0.9669 - val_loss: 1.5578 - val_accuracy: 0.6388\n",
      "Epoch 6/10\n",
      "750/750 [==============================] - 28s 38ms/step - loss: 0.0697 - accuracy: 0.9835 - val_loss: 1.6931 - val_accuracy: 0.6456\n",
      "Epoch 7/10\n",
      "750/750 [==============================] - 27s 36ms/step - loss: 0.0386 - accuracy: 0.9912 - val_loss: 1.9166 - val_accuracy: 0.6380\n",
      "Epoch 8/10\n",
      "750/750 [==============================] - 24s 32ms/step - loss: 0.0246 - accuracy: 0.9937 - val_loss: 2.0636 - val_accuracy: 0.6408\n",
      "Epoch 9/10\n",
      "750/750 [==============================] - 23s 31ms/step - loss: 0.0196 - accuracy: 0.9959 - val_loss: 2.1807 - val_accuracy: 0.6380\n",
      "Epoch 10/10\n",
      "750/750 [==============================] - 23s 31ms/step - loss: 0.0169 - accuracy: 0.9961 - val_loss: 2.2743 - val_accuracy: 0.6360\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Weighted F1-score: <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.624</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Weighted F1-score: \u001b[1;32m0.624\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def fit_and_evaluate(clf):\n",
    "    y_pred = fit_and_predict(clf)\n",
    "    wf1 = metrics.f1_score(df_val[\"topic\"], y_pred, average=\"weighted\")\n",
    "    color = \"green\" if wf1 >= .6 else \"red\"\n",
    "    rich.print(f\"Weighted F1-score: [bold {color}]{wf1:.3f}[/]\")\n",
    "\n",
    "fit_and_evaluate(make_neural_classifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oDGS9SvbfXAA",
   "metadata": {
    "id": "oDGS9SvbfXAA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4zIH5xa_Um3N",
   "metadata": {
    "id": "4zIH5xa_Um3N"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3dc4fae4-fc4e-4de7-9330-6516f58eebad",
   "metadata": {
    "id": "3dc4fae4-fc4e-4de7-9330-6516f58eebad"
   },
   "source": [
    "- - - \n",
    "\n",
    "## Part 3: Pre-trained word embeddings\n",
    "\n",
    "In this part, your task is to **integrate pre-trained word embeddings** into your neural classification model.\n",
    "\n",
    "In other words, use the exact same model that you made for Part 2, but change it so that it uses pre-trained word embeddings instead of randomly initialized embeddings. *(You might need to change the dimensionality of your embedding layer for this.)*\n",
    "\n",
    "Download and extract any pre-trained vectors from [GloVe](https://nlp.stanford.edu/projects/glove/) or [fastText](https://fasttext.cc/docs/en/english-vectors.html), then use the following function to load them into a dictionary object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1c800506-18cf-477e-9847-2dba4ca2616a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "dd2fd82c897642aa9ce4a8dfde15f4cb",
      "accb599458b4492994193aa472e19548"
     ]
    },
    "id": "1c800506-18cf-477e-9847-2dba4ca2616a",
    "outputId": "15df78de-8ca5-4d8c-bfd0-275772eccab9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd2fd82c897642aa9ce4a8dfde15f4cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Found <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">400001</span> vectors.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Found \u001b[1;36m400001\u001b[0m vectors.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_vectors(filename):\n",
    "    \"\"\"Loads word vectors in word2vec/Glove/fastText format.\"\"\"\n",
    "    vectors = {}\n",
    "    with rich.progress.open(filename, \"r\", description=\"Loading vectors...\") as f:\n",
    "        for line in f:\n",
    "            token, coefs = line.rstrip().split(maxsplit=1)\n",
    "            coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "            if len(coefs) < 2:  # probably a header\n",
    "                continue\n",
    "            vectors[token] = coefs\n",
    "    rich.print(f\"Found {len(vectors)} vectors.\")\n",
    "    return vectors\n",
    "\n",
    "#vectors = load_vectors(\"drive/MyDrive/glove/glove.6B.100d.txt\")\n",
    "vectors = load_vectors(\"glove/glove.6B.100d.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c18b5e-d670-4211-ab02-53ccf48f3976",
   "metadata": {
    "id": "69c18b5e-d670-4211-ab02-53ccf48f3976"
   },
   "source": [
    "<div style=\"background-color:#008148; padding:4px 8px; border-radius:4px; color:#F8F0E3; margin-bottom:1em\">\n",
    "    <strong>Modify the cell(s) below with your implementation.</strong>\n",
    "</div>\n",
    "\n",
    "Since you only need to redefine the model, the `fit_and_predict()` function from Part 2 should still work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ErPJRXtuCvLt",
   "metadata": {
    "id": "ErPJRXtuCvLt"
   },
   "outputs": [],
   "source": [
    "# The word_index is used to store the mapping from word to numbers in the dataset\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1M6sr_QQCvqH",
   "metadata": {
    "id": "1M6sr_QQCvqH"
   },
   "outputs": [],
   "source": [
    "# An embedding matrix for each word in dataset.\n",
    "# If the word has embedding in the GloVe, get its vector from GloVe\n",
    "# If not, take 0 instead\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, maxlen))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = vectors.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac5de28b-7263-43df-83f2-14dad890ddb8",
   "metadata": {
    "id": "ac5de28b-7263-43df-83f2-14dad890ddb8"
   },
   "outputs": [],
   "source": [
    "def make_neural_classifier_from_pretrained():\n",
    "    \"\"\"This function should instantiate and return your\n",
    "       neural text classification model, just like in Part 2,\n",
    "       except that this time you should use the `vectors`\n",
    "       loaded above.\"\"\"\n",
    "    #raise NotImplementedError()\n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(input_dim=len(word_index) + 1,\n",
    "                                output_dim=maxlen,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=maxlen,\n",
    "                                trainable=False))\n",
    "    model.add(layers.Conv1D(50, 5, activation=\"relu\"))\n",
    "    model.add(layers.GlobalMaxPool1D())\n",
    "    model.add(layers.Dense(10, activation='relu'))\n",
    "    model.add(layers.Dense(8, activation='softmax'))\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef31041f-42c6-49a7-85cf-bb385cf0d7bf",
   "metadata": {
    "id": "ef31041f-42c6-49a7-85cf-bb385cf0d7bf"
   },
   "source": [
    "<div style=\"background-color:#EF8A17; padding:4px 8px; border-radius:4px; color:#F8F0E3; margin-bottom:1em;\">\n",
    "  <strong>Do NOT modify the code cell below.</strong>\n",
    "</div>\n",
    "\n",
    "Run the cell below to fit and evaluate your model. Your goal is to implement a working neural classifier that **uses pre-trained embeddings**, still **trains in 10 minutes or less on CPU** and obtains **a weighted F1-score of 0.6 or more**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e164de8-cdb6-42d8-bf4e-1f9091ead519",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "id": "8e164de8-cdb6-42d8-bf4e-1f9091ead519",
    "outputId": "85cbe224-2f2e-4aaa-f7d5-d38322e24f5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "750/750 [==============================] - 13s 16ms/step - loss: 1.4677 - accuracy: 0.4839 - val_loss: 1.1494 - val_accuracy: 0.5984\n",
      "Epoch 2/10\n",
      "750/750 [==============================] - 12s 16ms/step - loss: 0.9576 - accuracy: 0.6748 - val_loss: 1.0459 - val_accuracy: 0.6392\n",
      "Epoch 3/10\n",
      "750/750 [==============================] - 12s 16ms/step - loss: 0.7502 - accuracy: 0.7472 - val_loss: 1.0468 - val_accuracy: 0.6356\n",
      "Epoch 4/10\n",
      "750/750 [==============================] - 16s 22ms/step - loss: 0.6038 - accuracy: 0.8040 - val_loss: 1.0826 - val_accuracy: 0.6396\n",
      "Epoch 5/10\n",
      "750/750 [==============================] - 19s 25ms/step - loss: 0.4802 - accuracy: 0.8451 - val_loss: 1.1800 - val_accuracy: 0.6300\n",
      "Epoch 6/10\n",
      "750/750 [==============================] - 22s 30ms/step - loss: 0.3773 - accuracy: 0.8880 - val_loss: 1.2140 - val_accuracy: 0.6260\n",
      "Epoch 7/10\n",
      "750/750 [==============================] - 24s 32ms/step - loss: 0.2919 - accuracy: 0.9163 - val_loss: 1.2975 - val_accuracy: 0.6256\n",
      "Epoch 8/10\n",
      "750/750 [==============================] - 22s 29ms/step - loss: 0.2274 - accuracy: 0.9400 - val_loss: 1.4281 - val_accuracy: 0.6136\n",
      "Epoch 9/10\n",
      "750/750 [==============================] - 24s 33ms/step - loss: 0.1761 - accuracy: 0.9556 - val_loss: 1.5802 - val_accuracy: 0.6156\n",
      "Epoch 10/10\n",
      "750/750 [==============================] - 23s 31ms/step - loss: 0.1375 - accuracy: 0.9660 - val_loss: 1.7092 - val_accuracy: 0.6060\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Weighted F1-score: <span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">0.609</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Weighted F1-score: \u001b[1;32m0.609\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fit_and_evaluate(make_neural_classifier_from_pretrained())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763f25f4-0eae-405f-88b0-d5f3badaf2d6",
   "metadata": {
    "id": "763f25f4-0eae-405f-88b0-d5f3badaf2d6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Cf3QQOP1Ht6E",
   "metadata": {
    "id": "Cf3QQOP1Ht6E"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "accb599458b4492994193aa472e19548": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd2fd82c897642aa9ce4a8dfde15f4cb": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_accb599458b4492994193aa472e19548",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Loading vectors... <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸</span> <span style=\"color: #008000; text-decoration-color: #008000\">346.3/347.1 MB</span> <span style=\"color: #008080; text-decoration-color: #008080\">0:00:01</span>\n</pre>\n",
         "text/plain": "Loading vectors... \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m \u001b[32m346.3/347.1 MB\u001b[0m \u001b[36m0:00:01\u001b[0m\n"
        },
        "metadata": {},
        "output_type": "display_data"
       }
      ]
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
